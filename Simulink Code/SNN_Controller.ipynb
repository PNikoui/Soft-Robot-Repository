{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hDnIEHOKB8LD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install snntorch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WL487gZW1Agy"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import snntorch as snn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYf13Gtx1OCj"
   },
   "source": [
    "## DataLoading\n",
    "Define variables for dataloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eo4T5MC21hgD"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "data_path='/SNNTrainingData.csv'\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myFKqNx11qYS"
   },
   "source": [
    "Load MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "csv_file = \"SNNTrainingData.csv\"\n",
    "csv_file2 = \"SNNTargetData.csv\"\n",
    "\n",
    "class CustomSNNDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        Inputs = pd.read_csv(csv_file2)\n",
    "        self.labels = np.array(Inputs)\n",
    "        self.images = np.array(data.iloc[:, 0:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-73eb95395820>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch =DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-64b9ed1dab4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCustomSNNDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SNNTargetData.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[0;32m    260\u001b[0m                     \u001b[1;31m# Cannot statically verify that dataset is Sized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[1;31m# Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[0;32m    104\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataset = CustomSNNDataset(\"SNNTargetData.csv\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "image_batch, label_batch = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3GdglZjK04cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'v', 'n', 'N', 'a', 'T', 'n', 'c', '.', 't', 'S', 'i', 'r', 'g', 'i']\n",
      "['s', 'D', 'N', 'a']\n"
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# # Define a transform\n",
    "# transform = transforms.Compose([\n",
    "#             transforms.Resize((28, 28)),\n",
    "#             transforms.Grayscale(),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "# mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(\"SNNTrainingData.csv\", test_size=0.2)\n",
    "print(train)\n",
    "print(val)\n",
    "# train.to_csv(\"train.csv\"), val.to_csv(\"val.csv\")\n",
    "\n",
    "# train_dataset = Roof_dataset(csv_file=\"train.csv\")  # Add any other params such as transforms here\n",
    "# val_dataset = Roof_dataset(csv_file=\"val.csv\") # Again add any other params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtJBOtez11wy"
   },
   "source": [
    "## Define Network with snnTorch. \n",
    "* `snn.Leaky()` instantiates a simple leaky integrate-and-fire neuron.\n",
    "* `spike_grad` optionally defines the surrogate gradient. If left undefined, the relevant gradient term is simply set to the output spike itself (1/0) by default.\n",
    "\n",
    "\n",
    "The problem with `nn.Sequential` is that each hidden layer can only pass one tensor to subsequent layers, whereas most spiking neurons return their spikes and hidden state(s). To handle this:\n",
    "\n",
    "* `init_hidden` initializes the hidden states (e.g., membrane potential) as instance variables to be processed in the background. \n",
    "\n",
    "The final layer is not bound by this constraint, and can return multiple tensors:\n",
    "* `output=True` enables the final layer to return the hidden state in addition to the spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JM2thnrc10rD"
   },
   "outputs": [],
   "source": [
    "from snntorch import surrogate\n",
    "\n",
    "beta = 0.9  # neuron decay rate \n",
    "spike_grad = surrogate.fast_sigmoid()\n",
    "\n",
    "#  Initialize Network\n",
    "net = nn.Sequential(snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(16*4*4, 10),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "                    ).to(device)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYSy5UuP4gXL"
   },
   "source": [
    "Refer to the snnTorch documentation to see more [neuron types](https://snntorch.readthedocs.io/en/latest/snntorch.html) and [surrogate gradient options](https://snntorch.readthedocs.io/en/latest/snntorch.surrogate.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIrJnBoz490c"
   },
   "source": [
    "## Define the Forward Pass\n",
    "Now define the forward pass over multiple time steps of simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hWa8f_We4-8z"
   },
   "outputs": [],
   "source": [
    "from snntorch import utils \n",
    "\n",
    "def forward_pass(net, data, num_steps):  \n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(num_steps): \n",
    "      spk_out, mem_out = net(data)\n",
    "      spk_rec.append(spk_out)\n",
    "  \n",
    "  return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nGhh2_25NU8"
   },
   "source": [
    "Define the optimizer and loss function. Here, we use the MSE Count Loss, which counts up the total number of output spikes at the end of the simulation run. The correct class has a target firing rate of 80% of all time steps, and incorrect classes are set to 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VocYbtD7Vwp7"
   },
   "outputs": [],
   "source": [
    "import snntorch.functional as SF\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWkx4ll761gU"
   },
   "source": [
    "Objective functions do not have to be applied to the spike count. They may be applied to the membrane potential (hidden state), or to spike-timing targets instead of rate-based methods. A non-exhaustive list of objective functions available include:\n",
    "\n",
    "**Apply the objective directly to spikes:**\n",
    "* MSE Spike Count Loss: `mse_count_loss()`\n",
    "* Cross Entropy Spike Count Loss: `ce_count_loss()`\n",
    "* Cross Entropy Spike Rate Loss: `ce_rate_loss()`\n",
    "\n",
    "**Apply the objective to the hidden state:**\n",
    "* Cross Entropy Maximum Membrane Potential Loss: `ce_max_membrane_loss()`\n",
    "* MSE Membrane Potential Loss: `mse_membrane_loss()`\n",
    "\n",
    "For alternative objective functions, refer to the `snntorch.functional` [documentation here.](https://snntorch.readthedocs.io/en/latest/snntorch.functional.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48_7sIT86iUJ"
   },
   "source": [
    "## Training Loop\n",
    "\n",
    "Now for the training loop. The predicted class will be set to the neuron with the highest firing rate, i.e., a rate-coded output. We will just measure accuracy on the training set. This training loop follows the same syntax as with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGZf7Hr55psl"
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "num_steps = 25  # run for 25 time steps \n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        spk_rec = forward_pass(net, data, num_steps)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # print every 25 iterations\n",
    "        if i % 25 == 0:\n",
    "          print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "\n",
    "          # check accuracy on a single batch\n",
    "          acc = SF.accuracy_rate(spk_rec, targets)  \n",
    "          acc_hist.append(acc)\n",
    "          print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "        \n",
    "        # uncomment for faster termination\n",
    "        # if i == 150:\n",
    "        #     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc0Yslzp7Z3M"
   },
   "source": [
    "## Automating Backprop\n",
    "\n",
    "Alternatively, we can automate the backprop through time training process using the `BPTT` method available in `snntorch.backprop`. All model updates take place within the `backprop.BPTT` function call. The specified number of steps in `num_steps` will be simulated just as before.\n",
    "\n",
    "> The following snippet will take some time to simulate; feel free to reduce the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfLQnvRQ7iXM"
   },
   "outputs": [],
   "source": [
    "from snntorch import backprop\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    avg_loss = backprop.BPTT(net, train_loader, num_steps=num_steps,\n",
    "                          optimizer=optimizer, criterion=loss_fn, time_var=False, device=device)\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {avg_loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-ilZc_G-AUE"
   },
   "source": [
    "Let's see the accuracy on the full test set, again using `SF.accuracy_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYXr5reY95Lc"
   },
   "outputs": [],
   "source": [
    "def test_accuracy(data_loader, net, num_steps):\n",
    "  with torch.no_grad():\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    net.eval()\n",
    "\n",
    "    data_loader = iter(data_loader)\n",
    "    for data, targets in data_loader:\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "      spk_rec = forward_pass(net, data, num_steps)\n",
    "\n",
    "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "      total += spk_rec.size(1)\n",
    "\n",
    "  return acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRRm1QpG9w7N"
   },
   "outputs": [],
   "source": [
    "print(f\"Test set accuracy: {test_accuracy(test_loader, net, num_steps)*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjOR18HA77gc"
   },
   "source": [
    "## More control over your model\n",
    "If you are simulating more complex architectures, such as residual nets, then your best bet is to wrap the network up in a class as shown below. This time, we will explicitly use the membrane potential, `mem`, and let `init_hidden` default to false.\n",
    "\n",
    "For the sake of speed, we'll just simulate a fully-connected SNN, but this can be generalized to other network types (e.g., Convs).\n",
    "\n",
    "In addition, let's set the neuron decay rate, `beta`, to be a learnable parameter. The first layer will have a shared decay rate across neurons. Each neuron in the second layer will have an independent decay rate. The decay is clipped between [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d286ef9-5fe6-4578-a686-91559a1f81d2"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        num_inputs = 784\n",
    "        num_hidden = 300\n",
    "        num_outputs = 10\n",
    "        spike_grad = surrogate.fast_sigmoid()\n",
    "\n",
    "        # global decay rate for all leaky neurons in layer 1\n",
    "        beta1 = 0.9\n",
    "        # independent decay rate for each leaky neuron in layer 2: [0, 1)\n",
    "        beta2 = torch.rand((num_outputs), dtype = torch.float) #.to(device)\n",
    "\n",
    "        # Init layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta1, spike_grad=spike_grad, learn_beta=True)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta2, spike_grad=spike_grad,learn_beta=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # reset hidden states and outputs at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x.flatten(1))\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec), torch.stack(mem2_rec)\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aCrVAh_cyTU"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "\n",
    "num_epochs = 1\n",
    "num_steps = 100  # run for 25 time steps \n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        spk_rec, _ = net(data)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # print every 25 iterations\n",
    "        if i % 25 == 0:\n",
    "          net.eval()\n",
    "          print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "\n",
    "          # check accuracy on a single batch\n",
    "          acc = SF.accuracy_rate(spk_rec, targets)  \n",
    "          acc_hist.append(acc)\n",
    "          print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "        \n",
    "        # uncomment for faster termination\n",
    "        # if i == 150:\n",
    "        #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmZJRdzIgpMb"
   },
   "outputs": [],
   "source": [
    "print(f\"Trained decay rate of the first layer: {net.lif1.beta:.3f}\\n\")\n",
    "\n",
    "print(f\"Trained decay rates of the second layer: {net.lif2.beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXCggOzk2vYF"
   },
   "outputs": [],
   "source": [
    "def test_accuracy(data_loader, net, num_steps):\n",
    "  with torch.no_grad():\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    net.eval()\n",
    "\n",
    "    data_loader = iter(data_loader)\n",
    "    for data, targets in data_loader:\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "      spk_rec, _ = net(data)\n",
    "\n",
    "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "      total += spk_rec.size(1)\n",
    "\n",
    "  return acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ias_TerdCMoG"
   },
   "outputs": [],
   "source": [
    "print(f\"Test set accuracy: {test_accuracy(test_loader, net, num_steps)*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iSGTq0Q3Lcm"
   },
   "source": [
    "# Conclusion\n",
    "That's it for the quick intro to snnTorch!\n",
    "\n",
    "* For a detailed tutorial of spiking neurons, neural nets, encoding, and training using neuromorphic datasets, check out the\n",
    "[snnTorch tutorial series](https://snntorch.readthedocs.io/en/latest/tutorials/index.html).\n",
    "* For more information on the features of snnTorch, check out the [documentation at this link](https://snntorch.readthedocs.io/en/latest/).\n",
    "* If you have ideas, suggestions or would like to find ways to get involved, then [check out the snnTorch GitHub project here.](https://github.com/jeshraghian/snntorch)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of tutorial_5_neuromorphic_datasets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
